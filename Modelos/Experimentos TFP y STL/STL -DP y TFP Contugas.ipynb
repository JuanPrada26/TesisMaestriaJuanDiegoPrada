{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_privacy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Contugas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrir datos contugas\n",
    "\n",
    "new1 = pd.read_excel('EICH106.xlsx')\n",
    "new1.columns = ['VOLUMEN CORREGIDO', 'STD_VOLUME', 'ORIG_TEMPERATURE', 'TEMPERATURA','PRESION', 'ORIG_PRESSURE', 'VOLUMENSINCORREGIR', 'RAW_VOLUME', 'FECHAINICIAL']\n",
    "\n",
    "#función que pone las fechas en el mismo formato\n",
    "def cambiofecha(row):\n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        if isinstance(row.at[i, 'FECHAINICIAL'], str):\n",
    "            row.at[i, 'FECHAINICIAL'] = pd.to_datetime(row.at[i, 'FECHAINICIAL']).strftime('%Y-%m-%d %H:%M:%S')  \n",
    "        elif isinstance(row.at[i, 'FECHAINICIAL'], datetime):\n",
    "            row.at[i, 'FECHAINICIAL'] = datetime.strptime(str(row.at[i, 'FECHAINICIAL']),'%Y-%d-%m %H:%M:%S')\n",
    "            row.at[i, 'FECHAINICIAL'] = row.at[i, 'FECHAINICIAL'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return row\n",
    "\n",
    "new1=cambiofecha(new1)\n",
    "\n",
    "new1 = new1.set_index('FECHAINICIAL')\n",
    "new1.index = pd.to_datetime(new1.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# Función que añade al dataframe la hora, dia de la semana, mes y dia del año.\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    #df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    #df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    #df['dayofmonth'] = df.index.day\n",
    "    #df['weekofyear'] = df.index.isocalendar().week\n",
    "    return df\n",
    "\n",
    "\n",
    "new1 = create_features(new1)\n",
    "\n",
    "datos1=new1[[\"PRESION\", \"TEMPERATURA\", \"VOLUMENSINCORREGIR\", \"hour\", \"dayofweek\", \"month\", \"dayofyear\"]]\n",
    "\n",
    "\n",
    "#función que elimina las anomalias \n",
    "\n",
    "def eliminar_anomalias(df1,Vol,VolMin,VolMax,Temp,TempMin,TempMax,Presion,PresMin,PresMax):\n",
    "\n",
    "    df=df1.copy()\n",
    "\n",
    "    if Vol == True:\n",
    "        df[\"VOLUMENSINCORREGIR\"]= np.where((df[\"VOLUMENSINCORREGIR\"]<VolMin)|(df[\"VOLUMENSINCORREGIR\"]>VolMax),df['VOLUMENSINCORREGIR'].shift(168),df['VOLUMENSINCORREGIR'])\n",
    "\n",
    "    if Presion == True:\n",
    "        df[\"PRESION\"]= np.where((df[\"PRESION\"]<PresMin)|(df[\"PRESION\"]>PresMax),df['PRESION'].shift(168),df['PRESION'])\n",
    "\n",
    "    if Temp == True:\n",
    "        df[\"TEMPERATURA\"]= np.where((df[\"TEMPERATURA\"]<TempMin)|(df[\"TEMPERATURA\"]>TempMax),df['TEMPERATURA'].shift(168),df['TEMPERATURA'])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "datos1=eliminar_anomalias(datos1,   True,0,250,True,17,35,True,14,19) ## Falta Presion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL - DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear función \n",
    "\n",
    "def STL_DP_S(datos,deltaf_p,b_p):\n",
    "\n",
    "    mstl = MSTL(datos, periods=[24, 24 * 7], iterate=5, stl_kwargs={\"seasonal_deg\": 0,\n",
    "                                                                            \"inner_iter\": 2,\n",
    "                                                                            \"outer_iter\": 0})\n",
    "    res = mstl.fit() # Use .fit() to perform and return the decomposition\n",
    "    #ax = res.plot()\n",
    "    #plt.tight_layout()\n",
    "\n",
    "\n",
    "    res.trend\n",
    "\n",
    "    tendencia = res.trend\n",
    "    seasonal = res.seasonal\n",
    "    residual = res.resid\n",
    "\n",
    "\n",
    "    tendenciaFourier = np.fft.fft(tendencia)\n",
    "\n",
    "    # Generar el ruido Laplaciano y aplicarlo a los coeficientes de Fourier\n",
    "    b = b_p\n",
    "    deltaf = deltaf_p\n",
    "    epsilon = deltaf/ b\n",
    "\n",
    "    # loc = media, scale = b\n",
    "    laplace = np.random.laplace(loc=0, scale=1/epsilon,size = tendenciaFourier.shape )\n",
    "\n",
    "    #laplace_noise = np.random.laplace(loc=0, scale=b, size=tendenciaFourier.shape)\n",
    "    perturbed_trend_dft = tendenciaFourier + laplace\n",
    "\n",
    "\n",
    "    # \n",
    "    perturbed_trend = np.fft.ifft(perturbed_trend_dft).real\n",
    "\n",
    "\n",
    "    # sacar datos de ruido\n",
    "    DatosRuido = perturbed_trend + seasonal['seasonal_168'] + seasonal['seasonal_24'] + residual\n",
    "\n",
    "\n",
    "\n",
    "    return DatosRuido,epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos11,e1 = STL_DP_S(datos1['TEMPERATURA'],1,100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos2,e2 = STL_DP_S(datos1['TEMPERATURA'],1,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos3,e3 = STL_DP_S(datos1['TEMPERATURA'],1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(datos3,label = \"e3\")\n",
    "plt.plot(datos11,label = \"e1\")\n",
    "plt.plot(datos2,label = \"e2\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,nombre):\n",
    "\n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = size_val, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'{nombre} Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas {batch} Batch.keras'\n",
    "    \n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train va a ser: (len(Datos) - ventana - prediccion ) * test_size\n",
    "\n",
    "# batch debe ser múltiplo de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,norm_clip,ruido,microBatches,lr,size_test,size_val,nombre):\n",
    "    \n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    print(\"Batch debe ser divisor de \", round((len(datos) - ventana - prediccion)* (1- size_test),1 ))\n",
    "\n",
    "    # batch debe ser divisor de X_train.shape[0]\n",
    "    \n",
    "    if X_train.shape[0] % batch == 0:\n",
    "        print(f\"El batch {batch} si sirve para el X_train {X_train.shape[0]}\")\n",
    "    else:\n",
    "        print(\"No Va a servir\")\n",
    "    #raise ValueError('Batch  No es divisor de X_Train')\n",
    "\n",
    "\n",
    "\n",
    "    # # micro batch debe ser múltiplo de batch \n",
    "\n",
    "    if batch % microBatches != 0:\n",
    "        raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "    else:\n",
    "        print(\"El MicroBatch si es múltiplo del batch\")\n",
    "\n",
    "\n",
    "\n",
    "    # # X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero. \n",
    "\n",
    "    if X_train.shape[0] * (1-size_val) % microBatches == 0:\n",
    "        print(\"X_train.shape[0] * (1-Val_size) / micro_batch es entero, va a servir.\")\n",
    "    else:\n",
    "        print(\"No va a servir porque X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # agregar la privacidad diferencial en el optimizador \n",
    "    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=norm_clip,\n",
    "        noise_multiplier=ruido,\n",
    "        num_microbatches=microBatches,\n",
    "        learning_rate=lr)\n",
    "\n",
    "    # Función de pérdida para regresión\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = size_val, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'{nombre} Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas {batch} Batch con TFP.keras'\n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metricas_Jd(y_hat,y_test,prediccion):\n",
    "    \n",
    "    predicciones_porHora_LSTM = []\n",
    "    reales_porHora_LSTM = []\n",
    "    for i in range(prediccion):\n",
    "            \n",
    "        pred = []\n",
    "        for Predicciones in y_hat:\n",
    "            pred.append(Predicciones[i])\n",
    "            \n",
    "        real = []\n",
    "        for reales in y_test:\n",
    "            real.append(reales[i])\n",
    "            \n",
    "        predicciones_porHora_LSTM.append(pred)\n",
    "        reales_porHora_LSTM.append(real)\n",
    "        \n",
    "\n",
    "    MAES_LSTM = {}\n",
    "    RMSE_LSTM = {}\n",
    "    ER_Medios_LSTM = {}\n",
    "    ER_Medianos_LSTM = {}\n",
    "    epsilon_LSTM = 1e-10\n",
    "    for i in range(prediccion):\n",
    "        MAE_LSTM = round(mean_absolute_error(predicciones_porHora_LSTM[i],reales_porHora_LSTM[i]),2)\n",
    "        MSE_LSTM = round(mean_squared_error(reales_porHora_LSTM[0],predicciones_porHora_LSTM[i]),2)\n",
    "        Error_Relativo_Medio_LSTM = round((np.mean(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        Error_Relativo_Mediano_LSTM = round((np.median(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        \n",
    "        \n",
    "        MAES_LSTM[i] = MAE_LSTM\n",
    "        RMSE_LSTM[i] = round(np.sqrt(MSE_LSTM),2)\n",
    "        ER_Medianos_LSTM[i] = Error_Relativo_Mediano_LSTM\n",
    "        ER_Medios_LSTM[i] = Error_Relativo_Medio_LSTM\n",
    "        \n",
    "    return MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrir modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Abrir_Modelo(datos,fechas,modelo,ventana,prediccion,size_test):\n",
    "        \n",
    "\n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = modelo.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat, y_test1,fecha_y_test,prediccion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función que haga todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tesis(datos,deltaf_p,b_p,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,nombre):\n",
    "\n",
    "\n",
    "    \n",
    "    # Realizar STL-DP\n",
    "    DatosRuido,epsilon_STL_DP = STL_DP_S(datos,deltaf_p,b_p)\n",
    "\n",
    "    print(f\"El Epsilon de STL es: {epsilon_STL_DP} \")\n",
    "    \n",
    "    # Realizar LSTM con TF_P\n",
    "    y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P,history_TF_P = LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,\"Contugas TFP\")\n",
    "\n",
    "\n",
    "    # Realizar LSTM con datos sin ruido\n",
    "    y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,\"Sin ruido\")\n",
    "    \n",
    "    # Realizar LSTM con datos STL_DP\n",
    "    y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,f\"STL _DP con epsilon {epsilon_STL_DP}\")\n",
    "\n",
    "    \n",
    "\n",
    "    MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM = Metricas_Jd(y_hatLSTM,y_testLSTM,prediccion)\n",
    "    MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP = Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n",
    "    MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P = Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)\n",
    "\n",
    "    Metricas = pd.DataFrame({'MAES_LSTM': MAES_LSTM, 'RMSE_LSTM': RMSE_LSTM, 'ER_Medianos_LSTM': ER_Medianos_LSTM,'ER_MEDIO_LSTM': ER_Medios_LSTM,\n",
    "                            'MAES_STL_DP': MAES_STL_DP, 'RMSE_STL_DP': RMSE_STL_DP, 'ER_Medianos_STL_DP': ER_Medianos_STL_DP,'ER_MEDIO_STL_DP': ER_Medios_STL_DP,\n",
    "                            'MAES_TF_P': MAES_TF_P, 'RMSE_TF_P': RMSE_TF_P, 'ER_Medianos_TF_P': ER_Medianos_TF_P,'ER_MEDIO_TF_P': ER_Medios_TF_P\n",
    "                            })\n",
    "\n",
    "\n",
    "\n",
    "    return y_hat_TF_P,y_test_TF_P,history_TF_P, y_hatLSTM,y_testLSTM, historyLSTM, y_hat_STL_DP,y_test_STL_DP, history_STL_DP, Metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar STL-DP a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos, DeltaF, B\n",
    "DatosRuido,epsilon = STL_DP_S(datos1['TEMPERATURA'],1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datos1['TEMPERATURA'][:168].values)\n",
    "plt.plot(DatosRuido.values[:168])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correr Red Neuronal LSTM Con Datos Perturbados y normales y correr TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 100\n",
    "nodos2 = 100\n",
    "paciencia = 10\n",
    "epocas = 20\n",
    "batch = 32\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "\n",
    "l2_norm_clip = 0\n",
    "noise_multiplier = 10\n",
    "num_microbatches = 4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM y real\n",
    "num = 0\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM, STL y Real\n",
    "\n",
    "num = 3\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_STL_DP[num],label = \"Predicción STL-DP\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train va a ser: (len(Datos) - ventana - prediccion ) * test_size\n",
    "\n",
    "# batch debe ser múltiplo de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero. \n",
    "\n",
    "\n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "paciencia = 10\n",
    "epocas = 20\n",
    "l2_norm_clip = 2\n",
    "noise_multiplier = 0.05\n",
    "num_microbatches = 3\n",
    "learning_rate = 0.001\n",
    "size_test = 0.2\n",
    "size_val = 0.2\n",
    "\n",
    "\n",
    "y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P,history_TF_P = LSTM_TFP_JD(datos[:],ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,\"Contugas TFP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM, STL,TFP y Real\n",
    "\n",
    "num = 5\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_STL_DP[num],label = \"Predicción STL-DP\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_TF_P[num],label = \"Predicción TF_P\")\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy_statement\n",
    "\n",
    "# Parámetros correctos para la función\n",
    "epsilon = compute_dp_sgd_privacy_statement(\n",
    "    number_of_examples = 35430, # Aquí usamos 'num_examples' en lugar de 'n'\n",
    "    batch_size=batch,\n",
    "    noise_multiplier= ruido,\n",
    "    delta=1e-5,\n",
    "    num_epochs = epocas\n",
    ")\n",
    "\n",
    "print(f\"Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch divisor de  (X - ventana - prediccion)\n",
    "#  batch debe ser divisor de  de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metricas_Jd(y_hat,y_test,prediccion):\n",
    "    \n",
    "    predicciones_porHora_LSTM = []\n",
    "    reales_porHora_LSTM = []\n",
    "    for i in range(prediccion):\n",
    "            \n",
    "        pred = []\n",
    "        for Predicciones in y_hat:\n",
    "            pred.append(Predicciones[i])\n",
    "            \n",
    "        real = []\n",
    "        for reales in y_test:\n",
    "            real.append(reales[i])\n",
    "            \n",
    "        predicciones_porHora_LSTM.append(pred)\n",
    "        reales_porHora_LSTM.append(real)\n",
    "        \n",
    "\n",
    "    MAES_LSTM = {}\n",
    "    RMSE_LSTM = {}\n",
    "    ER_Medios_LSTM = {}\n",
    "    ER_Medianos_LSTM = {}\n",
    "    epsilon_LSTM = 1e-10\n",
    "    for i in range(prediccion):\n",
    "        MAE_LSTM = round(mean_absolute_error(predicciones_porHora_LSTM[i],reales_porHora_LSTM[i]),2)\n",
    "        MSE_LSTM = round(mean_squared_error(reales_porHora_LSTM[0],predicciones_porHora_LSTM[i]),2)\n",
    "        Error_Relativo_Medio_LSTM = round((np.mean(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        Error_Relativo_Mediano_LSTM = round((np.median(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        \n",
    "        \n",
    "        MAES_LSTM[i] = MAE_LSTM\n",
    "        RMSE_LSTM[i] = round(np.sqrt(MSE_LSTM),2)\n",
    "        ER_Medianos_LSTM[i] = Error_Relativo_Mediano_LSTM\n",
    "        ER_Medios_LSTM[i] = Error_Relativo_Medio_LSTM\n",
    "        \n",
    "    return MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM = Metricas_Jd(y_hatLSTM,y_testLSTM,prediccion)\n",
    "MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP = Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n",
    "MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P = Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metricas = pd.DataFrame({'MAES_LSTM': MAES_LSTM, 'RMSE_LSTM': RMSE_LSTM, 'ER_Medianos_LSTM': ER_Medianos_LSTM,'ER_MEDIO_LSTM': ER_Medios_LSTM,\n",
    "                         'MAES_STL_DP': MAES_STL_DP, 'RMSE_STL_DP': RMSE_STL_DP, 'ER_Medianos_STL_DP': ER_Medianos_STL_DP,'ER_MEDIO_STL_DP': ER_Medios_STL_DP,\n",
    "                         'MAES_TF_P': MAES_TF_P, 'RMSE_TF_P': RMSE_TF_P, 'ER_Medianos_TF_P': ER_Medianos_TF_P,'ER_MEDIO_TF_P': ER_Medios_TF_P\n",
    "                         })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"MAE\")\n",
    "plt.plot(Metricas['MAES_LSTM'],label = \"LSTM\")\n",
    "plt.plot(Metricas['MAES_STL_DP'],label = \"STL_DP\")\n",
    "plt.plot(Metricas['MAES_TF_P'],label = \"TF_P\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacidad TF_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función que haga todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 100\n",
    "nodos2 = 100\n",
    "paciencia = 10\n",
    "epocas = 50\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "batch = 30\n",
    "l2_norm_clip = 2\n",
    "noise_multiplier = 0.05\n",
    "num_microbatches = 3\n",
    "learning_rate = 0.001\n",
    "size_test = 0.2\n",
    "size_val = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_TF_P,y_test_TF_P,history_TF_P, y_hatLSTM,y_testLSTM, historyLSTM, y_hat_STL_DP,y_test_STL_DP, history_STL_DP, Metricas = Tesis(datos,1,100,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_test,\"Prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir modelos y hacer predicciones y métricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrir Modelos Contugas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrir modelos \n",
    "\n",
    "modelo_lstm = load_model('Sin ruido Modelo 100 - nodos1 - 100 - nodos2 - 50 Epocas 30 Batch.keras')\n",
    "\n",
    "modelo_STL = load_model('STL _DP Modelo 100 - nodos1 - 100 - nodos2 - 50 Epocas 30 Batch.keras')\n",
    "\n",
    "#modelo_TFP = load_model('Contugas TFP Modelo 100 - nodos1 - 100 - nodos2 - 50 Epocas 30 Batch con TFP.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar X_test para \n",
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "fechas = datos1.index\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "size_test = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_hatLSTM, y_testLSTM,fecha_y_testLSTM,prediccion_LSTM = Abrir_Modelo(datos,fechas,modelo_lstm,ventana,prediccion,size_test)\n",
    "y_hat_STL_DP, y_test_STL_DP,fecha_y_test_STL_DP,prediccion_STL_DP = Abrir_Modelo(datos,fechas,modelo_STL,ventana,prediccion,size_test)\n",
    "y_hat_TF_P, y_test_TF_P,fecha_y_test_TF_P,prediccion_TF_P = Abrir_Modelo(datos,fechas,modelo_STL,ventana,prediccion,size_test)\n",
    "\n",
    "\n",
    "## Sacar métricas\n",
    "\n",
    "MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM = Metricas_Jd(y_hatLSTM,y_testLSTM,prediccion)\n",
    "MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP = Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n",
    "MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P = Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)\n",
    "\n",
    "Metricas = pd.DataFrame({'MAES_LSTM': MAES_LSTM, 'RMSE_LSTM': RMSE_LSTM, 'ER_Medianos_LSTM': ER_Medianos_LSTM,'ER_MEDIO_LSTM': ER_Medios_LSTM,\n",
    "                        'MAES_STL_DP': MAES_STL_DP, 'RMSE_STL_DP': RMSE_STL_DP, 'ER_Medianos_STL_DP': ER_Medianos_STL_DP,'ER_MEDIO_STL_DP': ER_Medios_STL_DP,\n",
    "                        'MAES_TF_P': MAES_TF_P, 'RMSE_TF_P': RMSE_TF_P, 'ER_Medianos_TF_P': ER_Medianos_TF_P,'ER_MEDIO_TF_P': ER_Medios_TF_P\n",
    "                        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2,figsize = (15,7.5))\n",
    "fig.suptitle(\"Métricas\")\n",
    "\n",
    "\n",
    "\n",
    "ax[0,0].set_title(\"Comparación MAES\")\n",
    "ax[0,0].plot(Metricas['MAES_LSTM'],label = \"LSTM\")\n",
    "ax[0,0].plot(Metricas['MAES_STL_DP'],label = \"STL_DP\")\n",
    "#ax[0,0].plot(Metricas['MAES_TF_P'],label = \"TF_P\")\n",
    "ax[0,0].legend()\n",
    "ax[0,0].grid()\n",
    "\n",
    "\n",
    "ax[0,1].set_title(\"Comparación ER_MEDIO\")\n",
    "ax[0,1].plot(Metricas['ER_MEDIO_LSTM'],label = \"LSTM\")\n",
    "ax[0,1].plot(Metricas['ER_MEDIO_STL_DP'],label = \"STL_DP\")\n",
    "#ax[0,1].plot(Metricas['ER_MEDIO_TF_P'],label = \"TF_P\")\n",
    "ax[0,1].legend()\n",
    "ax[0,1].grid()\n",
    "\n",
    "\n",
    "ax[1,0].set_title(\"Comparación RMSE\")\n",
    "ax[1,0].plot(Metricas['RMSE_LSTM'],label = \"LSTM\")\n",
    "ax[1,0].plot(Metricas['RMSE_STL_DP'],label = \"STL_DP\")\n",
    "#ax[1,0].plot(Metricas['RMSE_TF_P'],label = \"TF_P\")\n",
    "ax[1,0].legend()\n",
    "ax[1,0].grid()\n",
    "\n",
    "\n",
    "\n",
    "ax[1,1].set_title(\"Comparación ER_MEDIANO\")\n",
    "ax[1,1].plot(Metricas['ER_Medianos_LSTM'],label = \"LSTM\")\n",
    "ax[1,1].plot(Metricas['ER_Medianos_STL_DP'],label = \"STL_DP\")\n",
    "#ax[1,1].plot(Metricas['ER_Medianos_TF_P'],label = \"TF_P\")\n",
    "ax[1,1].legend()\n",
    "ax[1,1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM, STL,TFP y Real\n",
    "\n",
    "num = 200\n",
    "\n",
    "\n",
    "plt.plot(fecha_y_test_STL_DP[num],y_hat_STL_DP[num],label = \"Predicción STL_DP\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin Ruido\")\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "#from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "# Registrar el optimizador personalizado\n",
    "get_custom_objects().update({\"DPKerasSGDOptimizer\": DPKerasSGDOptimizer})\n",
    "\n",
    "# Cargar el modelo con el optimizador de privacidad\n",
    "modelo_TFP = load_model('Contugas TFP Modelo 100 - nodos1 - 100 - nodos2 - 50 Epocas 30 Batch con TFP.keras', custom_objects={\"DPKerasSGDOptimizer\": tensorflow_privacy.DPKerasSGDOptimizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "\n",
    "def LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,norm_clip,ruido,microBatches,lr,size_test,size_val,nombre):\n",
    "    \n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    print(\"Batch debe ser divisor de \", round((len(datos) - ventana - prediccion)* (1- size_test),1 ))\n",
    "\n",
    "    # batch debe ser divisor de X_train.shape[0]\n",
    "    \n",
    "    if X_train.shape[0] % batch == 0:\n",
    "        print(f\"El batch {batch} si sirve para el X_train {X_train.shape[0]}\")\n",
    "    else:\n",
    "        print(\"No Va a servir\")\n",
    "    #raise ValueError('Batch  No es divisor de X_Train')\n",
    "\n",
    "\n",
    "\n",
    "    # # micro batch debe ser múltiplo de batch \n",
    "\n",
    "    if batch % microBatches != 0:\n",
    "        raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "    else:\n",
    "        print(\"El MicroBatch si es múltiplo del batch\")\n",
    "\n",
    "\n",
    "\n",
    "    # # X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero. \n",
    "\n",
    "    if X_train.shape[0] * (1-size_val) % microBatches == 0:\n",
    "        print(\"X_train.shape[0] * (1-Val_size) / micro_batch es entero, va a servir.\")\n",
    "    else:\n",
    "        print(\"No va a servir porque X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # agregar la privacidad diferencial en el optimizador \n",
    "    optimizer = DPKerasAdamOptimizer(\n",
    "        l2_norm_clip=norm_clip,\n",
    "        noise_multiplier=ruido,\n",
    "        num_microbatches=microBatches,\n",
    "        learning_rate=lr)\n",
    "\n",
    "    # Función de pérdida para regresión\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = size_val, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'{nombre} Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas {batch} Batch con TFP.keras'\n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "    # Guarda el modelo como archivo HDF5 o SavedModel\n",
    "    model.save('modelo_TFP.h5')  # o modelo_TFP.save('modelo_TFP')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 2\n",
    "nodos2 = 2\n",
    "paciencia = 10\n",
    "epocas = 3\n",
    "batch = 32\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "\n",
    "l2_norm_clip = 0\n",
    "noise_multiplier = 10\n",
    "num_microbatches = 4\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "paciencia = 10\n",
    "epocas = 2\n",
    "l2_norm_clip = 2\n",
    "noise_multiplier = 0.05\n",
    "num_microbatches = 3\n",
    "learning_rate = 0.001\n",
    "size_test = 0.2\n",
    "size_val = 0.2\n",
    "\n",
    "\n",
    "y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P,history_TF_P = LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,\"Contugas TFP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "# Cargar el modelo de archivo HDF5 o SavedModel\n",
    "modelo_TFP = tf.keras.models.load_model('modelo_TFP.h5')  # o la ruta de SavedModel\n",
    "\n",
    "# # Guardar el modelo en formato pickle\n",
    "# with open('modelo_TFP.pkl', 'wb') as f:\n",
    "#     pickle.dump(modelo_TFP, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_TFP = load_model('Contugas TFP Modelo 2 - nodos1 - 2 - nodos2 - 2 Epocas 30 Batch con TFP.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Contugas-m7l_E1hd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
