{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_privacy\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Contugas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrir datos contugas\n",
    "\n",
    "new1 = pd.read_excel('EICH106.xlsx')\n",
    "new1.columns = ['VOLUMEN CORREGIDO', 'STD_VOLUME', 'ORIG_TEMPERATURE', 'TEMPERATURA','PRESION', 'ORIG_PRESSURE', 'VOLUMENSINCORREGIR', 'RAW_VOLUME', 'FECHAINICIAL']\n",
    "\n",
    "#función que pone las fechas en el mismo formato\n",
    "def cambiofecha(row):\n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        if isinstance(row.at[i, 'FECHAINICIAL'], str):\n",
    "            row.at[i, 'FECHAINICIAL'] = pd.to_datetime(row.at[i, 'FECHAINICIAL']).strftime('%Y-%m-%d %H:%M:%S')  \n",
    "        elif isinstance(row.at[i, 'FECHAINICIAL'], datetime):\n",
    "            row.at[i, 'FECHAINICIAL'] = datetime.strptime(str(row.at[i, 'FECHAINICIAL']),'%Y-%d-%m %H:%M:%S')\n",
    "            row.at[i, 'FECHAINICIAL'] = row.at[i, 'FECHAINICIAL'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return row\n",
    "\n",
    "new1=cambiofecha(new1)\n",
    "\n",
    "new1 = new1.set_index('FECHAINICIAL')\n",
    "new1.index = pd.to_datetime(new1.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# Función que añade al dataframe la hora, dia de la semana, mes y dia del año.\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    #df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    #df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    #df['dayofmonth'] = df.index.day\n",
    "    #df['weekofyear'] = df.index.isocalendar().week\n",
    "    return df\n",
    "\n",
    "\n",
    "new1 = create_features(new1)\n",
    "\n",
    "datos1=new1[[\"PRESION\", \"TEMPERATURA\", \"VOLUMENSINCORREGIR\", \"hour\", \"dayofweek\", \"month\", \"dayofyear\"]]\n",
    "\n",
    "\n",
    "#función que elimina las anomalias \n",
    "\n",
    "def eliminar_anomalias(df1,Vol,VolMin,VolMax,Temp,TempMin,TempMax,Presion,PresMin,PresMax):\n",
    "\n",
    "    df=df1.copy()\n",
    "\n",
    "    if Vol == True:\n",
    "        df[\"VOLUMENSINCORREGIR\"]= np.where((df[\"VOLUMENSINCORREGIR\"]<VolMin)|(df[\"VOLUMENSINCORREGIR\"]>VolMax),df['VOLUMENSINCORREGIR'].shift(168),df['VOLUMENSINCORREGIR'])\n",
    "\n",
    "    if Presion == True:\n",
    "        df[\"PRESION\"]= np.where((df[\"PRESION\"]<PresMin)|(df[\"PRESION\"]>PresMax),df['PRESION'].shift(168),df['PRESION'])\n",
    "\n",
    "    if Temp == True:\n",
    "        df[\"TEMPERATURA\"]= np.where((df[\"TEMPERATURA\"]<TempMin)|(df[\"TEMPERATURA\"]>TempMax),df['TEMPERATURA'].shift(168),df['TEMPERATURA'])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "datos1=eliminar_anomalias(datos1,   True,0,250,True,17,35,True,14,19) ## Falta Presion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL - DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear función \n",
    "\n",
    "def STL_DP_S(datos,deltaf_p,b_p):\n",
    "\n",
    "    mstl = MSTL(datos, periods=[24, 24 * 7], iterate=5, stl_kwargs={\"seasonal_deg\": 0,\n",
    "                                                                            \"inner_iter\": 2,\n",
    "                                                                            \"outer_iter\": 0})\n",
    "    res = mstl.fit() # Use .fit() to perform and return the decomposition\n",
    "    #ax = res.plot()\n",
    "    #plt.tight_layout()\n",
    "\n",
    "\n",
    "    res.trend\n",
    "\n",
    "    tendencia = res.trend\n",
    "    seasonal = res.seasonal\n",
    "    residual = res.resid\n",
    "\n",
    "\n",
    "    tendenciaFourier = np.fft.fft(tendencia)\n",
    "\n",
    "    # Generar el ruido Laplaciano y aplicarlo a los coeficientes de Fourier\n",
    "    b = b_p\n",
    "    deltaf = deltaf_p\n",
    "    epsilon = deltaf/ b\n",
    "\n",
    "    # loc = media, scale = b\n",
    "    laplace = np.random.laplace(loc=0, scale=1/epsilon,size = tendenciaFourier.shape )\n",
    "\n",
    "    #laplace_noise = np.random.laplace(loc=0, scale=b, size=tendenciaFourier.shape)\n",
    "    perturbed_trend_dft = tendenciaFourier + laplace\n",
    "\n",
    "\n",
    "    # \n",
    "    perturbed_trend = np.fft.ifft(perturbed_trend_dft).real\n",
    "\n",
    "\n",
    "    # sacar datos de ruido\n",
    "    DatosRuido = perturbed_trend + seasonal['seasonal_168'] + seasonal['seasonal_24'] + residual\n",
    "\n",
    "\n",
    "\n",
    "    return DatosRuido,epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,nombre):\n",
    "\n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = size_val, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'{nombre} Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas {batch} Batch.keras'\n",
    "    \n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train va a ser: (len(Datos) - ventana - prediccion ) * test_size\n",
    "\n",
    "# batch debe ser múltiplo de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,norm_clip,ruido,microBatches,lr,size_test,size_val,nombre):\n",
    "    \n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size_test, shuffle=False)\n",
    "\n",
    "    print(\"Batch debe ser divisor de \", round((len(datos) - ventana - prediccion)* (1- size_test),1 ))\n",
    "\n",
    "    # batch debe ser divisor de X_train.shape[0]\n",
    "    \n",
    "    if X_train.shape[0] % batch == 0:\n",
    "        print(f\"El batch {batch} si sirve para el X_train {X_train.shape[0]}\")\n",
    "    else:\n",
    "        print(\"No Va a servir\")\n",
    "    #raise ValueError('Batch  No es divisor de X_Train')\n",
    "\n",
    "\n",
    "\n",
    "    # # micro batch debe ser múltiplo de batch \n",
    "\n",
    "    if batch % microBatches != 0:\n",
    "        raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "    else:\n",
    "        print(\"El MicroBatch si es múltiplo del batch\")\n",
    "\n",
    "\n",
    "\n",
    "    # # X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero. \n",
    "\n",
    "    if X_train.shape[0] * (1-size_val) % microBatches == 0:\n",
    "        print(\"X_train.shape[0] * (1-Val_size) / micro_batch es entero, va a servir.\")\n",
    "    else:\n",
    "        print(\"No va a servir porque X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # agregar la privacidad diferencial en el optimizador \n",
    "    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=norm_clip,\n",
    "        noise_multiplier=ruido,\n",
    "        num_microbatches=microBatches,\n",
    "        learning_rate=lr)\n",
    "\n",
    "    # Función de pérdida para regresión\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = size_val, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'{nombre} Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas {batch} Batch con TFP.keras'\n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar STL-DP a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos, DeltaF, B\n",
    "DatosRuido,epsilon = STL_DP_S(datos1['TEMPERATURA'],1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datos1['TEMPERATURA'][:168].values)\n",
    "plt.plot(DatosRuido.values[:168])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correr Red Neuronal LSTM Con Datos Perturbados y normales y correr TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 100\n",
    "nodos2 = 100\n",
    "paciencia = 10\n",
    "epocas = 20\n",
    "batch = 32\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "\n",
    "l2_norm_clip = 0\n",
    "noise_multiplier = 10\n",
    "num_microbatches = 4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM y real\n",
    "num = 0\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM, STL y Real\n",
    "\n",
    "num = 3\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_STL_DP[num],label = \"Predicción STL-DP\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train va a ser: (len(Datos) - ventana - prediccion ) * test_size\n",
    "\n",
    "# batch debe ser múltiplo de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# X_train.shape[0] * (1-Val_size) / micro_batch debe ser entero. \n",
    "\n",
    "\n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "paciencia = 10\n",
    "epocas = 20\n",
    "l2_norm_clip = 2\n",
    "noise_multiplier = 0.05\n",
    "num_microbatches = 3\n",
    "learning_rate = 0.001\n",
    "size_test = 0.2\n",
    "size_val = 0.2\n",
    "\n",
    "\n",
    "y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P,history_TF_P = LSTM_TFP_JD(datos[:],ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,\"Contugas TFP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafica LSTM, STL,TFP y Real\n",
    "\n",
    "num = 5\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_hatLSTM[num],label = \"Predicción Sin ruido\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_STL_DP[num],label = \"Predicción STL-DP\")\n",
    "plt.plot(fecha_y_testLSTM[num],y_hat_TF_P[num],label = \"Predicción TF_P\")\n",
    "\n",
    "plt.plot(fecha_y_testLSTM[num],y_testLSTM[num],label = \"Datos Reales\")\n",
    "\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy_statement\n",
    "\n",
    "# Parámetros correctos para la función\n",
    "epsilon = compute_dp_sgd_privacy_statement(\n",
    "    number_of_examples = 35430, # Aquí usamos 'num_examples' en lugar de 'n'\n",
    "    batch_size=batch,\n",
    "    noise_multiplier= ruido,\n",
    "    delta=1e-5,\n",
    "    num_epochs = epocas\n",
    ")\n",
    "\n",
    "print(f\"Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch divisor de  (X - ventana - prediccion)\n",
    "#  batch debe ser divisor de  de X_train.shape[0]\n",
    "# micro batch debe ser múltiplo de batch \n",
    "# de donde sale el número abajo de época cuando corro el modelo? \n",
    "# número de batches que el modelo procesa en cada época.\n",
    "# se calcula como X_train.shape[0] * (1-Val_size) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metricas_Jd(y_hat,y_test,prediccion):\n",
    "    \n",
    "    predicciones_porHora_LSTM = []\n",
    "    reales_porHora_LSTM = []\n",
    "    for i in range(prediccion):\n",
    "            \n",
    "        pred = []\n",
    "        for Predicciones in y_hat:\n",
    "            pred.append(Predicciones[i])\n",
    "            \n",
    "        real = []\n",
    "        for reales in y_test:\n",
    "            real.append(reales[i])\n",
    "            \n",
    "        predicciones_porHora_LSTM.append(pred)\n",
    "        reales_porHora_LSTM.append(real)\n",
    "        \n",
    "\n",
    "    MAES_LSTM = {}\n",
    "    RMSE_LSTM = {}\n",
    "    ER_Medios_LSTM = {}\n",
    "    ER_Medianos_LSTM = {}\n",
    "    epsilon_LSTM = 1e-10\n",
    "    for i in range(prediccion):\n",
    "        MAE_LSTM = round(mean_absolute_error(predicciones_porHora_LSTM[i],reales_porHora_LSTM[i]),2)\n",
    "        MSE_LSTM = round(mean_squared_error(reales_porHora_LSTM[0],predicciones_porHora_LSTM[i]),2)\n",
    "        Error_Relativo_Medio_LSTM = round((np.mean(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        Error_Relativo_Mediano_LSTM = round((np.median(np.abs((np.array(reales_porHora_LSTM[i]) - np.array(predicciones_porHora_LSTM[i])) / (np.array(reales_porHora_LSTM[i])+epsilon_LSTM)))*100),2)\n",
    "        \n",
    "        \n",
    "        MAES_LSTM[i] = MAE_LSTM\n",
    "        RMSE_LSTM[i] = round(np.sqrt(MSE_LSTM),2)\n",
    "        ER_Medianos_LSTM[i] = Error_Relativo_Mediano_LSTM\n",
    "        ER_Medios_LSTM[i] = Error_Relativo_Medio_LSTM\n",
    "        \n",
    "    return MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM = Metricas_Jd(y_hatLSTM,y_testLSTM,prediccion)\n",
    "MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP = Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n",
    "MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P = Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metricas = pd.DataFrame({'MAES_LSTM': MAES_LSTM, 'RMSE_LSTM': RMSE_LSTM, 'ER_Medianos_LSTM': ER_Medianos_LSTM,'ER_MEDIO_LSTM': ER_Medios_LSTM,\n",
    "                         'MAES_STL_DP': MAES_STL_DP, 'RMSE_STL_DP': RMSE_STL_DP, 'ER_Medianos_STL_DP': ER_Medianos_STL_DP,'ER_MEDIO_STL_DP': ER_Medios_STL_DP,\n",
    "                         'MAES_TF_P': MAES_TF_P, 'RMSE_TF_P': RMSE_TF_P, 'ER_Medianos_TF_P': ER_Medianos_TF_P,'ER_MEDIO_TF_P': ER_Medios_TF_P\n",
    "                         })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"MAE\")\n",
    "plt.plot(Metricas['MAES_LSTM'],label = \"LSTM\")\n",
    "plt.plot(Metricas['MAES_STL_DP'],label = \"STL_DP\")\n",
    "plt.plot(Metricas['MAES_TF_P'],label = \"TF_P\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacidad TF_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función que haga todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tesis(datos,deltaf_p,b_p,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,nombre):\n",
    "\n",
    "\n",
    "    \n",
    "    # Realizar STL-DP\n",
    "    DatosRuido,epsilon_STL_DP = STL_DP_S(datos,deltaf_p,b_p)\n",
    "\n",
    "    print(f\"El Epsilon de STL es: {epsilon_STL_DP} \")\n",
    "    \n",
    "    # Realizar LSTM con TF_P\n",
    "    y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P,history_TF_P = LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_val,\"Contugas TFP\")\n",
    "\n",
    "\n",
    "    # Realizar LSTM con datos sin ruido\n",
    "    y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,\"Sin ruido\")\n",
    "    \n",
    "    # Realizar LSTM con datos STL_DP\n",
    "    y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,\"STL _DP\")\n",
    "\n",
    "    \n",
    "\n",
    "    MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM = Metricas_Jd(y_hatLSTM,y_testLSTM,prediccion)\n",
    "    MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP = Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n",
    "    MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P = Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)\n",
    "\n",
    "    Metricas = pd.DataFrame({'MAES_LSTM': MAES_LSTM, 'RMSE_LSTM': RMSE_LSTM, 'ER_Medianos_LSTM': ER_Medianos_LSTM,'ER_MEDIO_LSTM': ER_Medios_LSTM,\n",
    "                            'MAES_STL_DP': MAES_STL_DP, 'RMSE_STL_DP': RMSE_STL_DP, 'ER_Medianos_STL_DP': ER_Medianos_STL_DP,'ER_MEDIO_STL_DP': ER_Medios_STL_DP,\n",
    "                            'MAES_TF_P': MAES_TF_P, 'RMSE_TF_P': RMSE_TF_P, 'ER_Medianos_TF_P': ER_Medianos_TF_P,'ER_MEDIO_TF_P': ER_Medios_TF_P\n",
    "                            })\n",
    "\n",
    "\n",
    "\n",
    "    return y_hat_TF_P,y_test_TF_P,history_TF_P, y_hatLSTM,y_testLSTM, historyLSTM, y_hat_STL_DP,y_test_STL_DP, history_STL_DP, Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "datos = datos1['TEMPERATURA']\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 100\n",
    "nodos2 = 100\n",
    "paciencia = 10\n",
    "epocas = 50\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "batch = 30\n",
    "l2_norm_clip = 2\n",
    "noise_multiplier = 0.05\n",
    "num_microbatches = 3\n",
    "learning_rate = 0.001\n",
    "size_test = 0.2\n",
    "size_val = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Epsilon de STL es: 0.01 \n",
      "Batch debe ser divisor de  35430.4\n",
      "El batch 30 si sirve para el X_train 35430\n",
      "El MicroBatch si es múltiplo del batch\n",
      "X_train.shape[0] * (1-Val_size) / micro_batch es entero, va a servir.\n",
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\jd.pradal\\.virtualenvs\\Contugas-m7l_E1hd\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "945/945 [==============================] - 121s 118ms/step - loss: 0.2013 - val_loss: 0.0860\n",
      "Epoch 2/50\n",
      "945/945 [==============================] - 114s 121ms/step - loss: 0.1153 - val_loss: 0.0492\n",
      "Epoch 3/50\n",
      "945/945 [==============================] - 116s 123ms/step - loss: 0.0663 - val_loss: 0.0370\n",
      "Epoch 4/50\n",
      "945/945 [==============================] - 117s 124ms/step - loss: 0.0443 - val_loss: 0.0370\n",
      "Epoch 5/50\n",
      "945/945 [==============================] - 118s 125ms/step - loss: 0.0366 - val_loss: 0.0382\n",
      "Epoch 6/50\n",
      "945/945 [==============================] - 119s 126ms/step - loss: 0.0338 - val_loss: 0.0386\n",
      "Epoch 7/50\n",
      "945/945 [==============================] - 119s 126ms/step - loss: 0.0323 - val_loss: 0.0379\n",
      "Epoch 8/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0311 - val_loss: 0.0370\n",
      "Epoch 9/50\n",
      "945/945 [==============================] - 120s 128ms/step - loss: 0.0300 - val_loss: 0.0357\n",
      "Epoch 10/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0289 - val_loss: 0.0343\n",
      "Epoch 11/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0277 - val_loss: 0.0330\n",
      "Epoch 12/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0266 - val_loss: 0.0314\n",
      "Epoch 13/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0253 - val_loss: 0.0298\n",
      "Epoch 14/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0241 - val_loss: 0.0281\n",
      "Epoch 15/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0227 - val_loss: 0.0263\n",
      "Epoch 16/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0213 - val_loss: 0.0245\n",
      "Epoch 17/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0199 - val_loss: 0.0226\n",
      "Epoch 18/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0184 - val_loss: 0.0207\n",
      "Epoch 19/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0169 - val_loss: 0.0188\n",
      "Epoch 20/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0154 - val_loss: 0.0170\n",
      "Epoch 21/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0141 - val_loss: 0.0155\n",
      "Epoch 22/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0129 - val_loss: 0.0140\n",
      "Epoch 23/50\n",
      "945/945 [==============================] - 122s 129ms/step - loss: 0.0119 - val_loss: 0.0128\n",
      "Epoch 24/50\n",
      "945/945 [==============================] - 123s 131ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 25/50\n",
      "945/945 [==============================] - 123s 130ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 26/50\n",
      "945/945 [==============================] - 121s 129ms/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 27/50\n",
      "945/945 [==============================] - 123s 130ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 28/50\n",
      "945/945 [==============================] - 122s 129ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 29/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 30/50\n",
      "945/945 [==============================] - 125s 132ms/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 31/50\n",
      "945/945 [==============================] - 122s 129ms/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 32/50\n",
      "945/945 [==============================] - 123s 130ms/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 33/50\n",
      "945/945 [==============================] - 121s 129ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 34/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 35/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 36/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 37/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 38/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 39/50\n",
      "945/945 [==============================] - 120s 127ms/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 40/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 41/50\n",
      "945/945 [==============================] - 121s 128ms/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 42/50\n",
      "945/945 [==============================] - 125s 132ms/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 43/50\n",
      "945/945 [==============================] - 146s 155ms/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 44/50\n",
      "945/945 [==============================] - 164s 173ms/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 45/50\n",
      "945/945 [==============================] - 169s 179ms/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 46/50\n",
      "945/945 [==============================] - 170s 180ms/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 47/50\n",
      "945/945 [==============================] - 169s 179ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 48/50\n",
      "945/945 [==============================] - 180s 190ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 49/50\n",
      "945/945 [==============================] - 182s 193ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 50/50\n",
      "945/945 [==============================] - 166s 176ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "277/277 [==============================] - 8s 27ms/step\n",
      "Epoch 1/50\n",
      "945/945 [==============================] - 71s 73ms/step - loss: 0.0109 - val_loss: 0.0350\n",
      "Epoch 2/50\n",
      "945/945 [==============================] - 73s 78ms/step - loss: 0.0081 - val_loss: 0.0351\n",
      "Epoch 3/50\n",
      "945/945 [==============================] - 85s 90ms/step - loss: 0.0078 - val_loss: 0.0519\n",
      "Epoch 4/50\n",
      "945/945 [==============================] - 94s 99ms/step - loss: 0.0077 - val_loss: 0.0486\n",
      "Epoch 5/50\n",
      "945/945 [==============================] - 89s 94ms/step - loss: 0.0076 - val_loss: 0.0295\n",
      "Epoch 6/50\n",
      "945/945 [==============================] - 101s 107ms/step - loss: 0.0074 - val_loss: 0.0235\n",
      "Epoch 7/50\n",
      "945/945 [==============================] - 98s 103ms/step - loss: 0.0073 - val_loss: 0.0243\n",
      "Epoch 8/50\n",
      "945/945 [==============================] - 93s 99ms/step - loss: 0.0073 - val_loss: 0.0252\n",
      "Epoch 9/50\n",
      "945/945 [==============================] - 81s 85ms/step - loss: 0.0072 - val_loss: 0.0253\n",
      "Epoch 10/50\n",
      "945/945 [==============================] - 74s 79ms/step - loss: 0.0072 - val_loss: 0.0267\n",
      "Epoch 11/50\n",
      "945/945 [==============================] - 62s 66ms/step - loss: 0.0072 - val_loss: 0.0281\n",
      "Epoch 12/50\n",
      "945/945 [==============================] - 55s 58ms/step - loss: 0.0071 - val_loss: 0.0280\n",
      "Epoch 13/50\n",
      "945/945 [==============================] - 62s 65ms/step - loss: 0.0071 - val_loss: 0.0254\n",
      "Epoch 14/50\n",
      "945/945 [==============================] - 86s 91ms/step - loss: 0.0071 - val_loss: 0.0249\n",
      "Epoch 15/50\n",
      "945/945 [==============================] - 87s 92ms/step - loss: 0.0071 - val_loss: 0.0217\n",
      "Epoch 16/50\n",
      "945/945 [==============================] - 72s 76ms/step - loss: 0.0070 - val_loss: 0.0212\n",
      "Epoch 17/50\n",
      "945/945 [==============================] - 88s 93ms/step - loss: 0.0070 - val_loss: 0.0195\n",
      "Epoch 18/50\n",
      "945/945 [==============================] - 84s 89ms/step - loss: 0.0070 - val_loss: 0.0185\n",
      "Epoch 19/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0070 - val_loss: 0.0177\n",
      "Epoch 20/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0069 - val_loss: 0.0174\n",
      "Epoch 21/50\n",
      "945/945 [==============================] - 63s 67ms/step - loss: 0.0069 - val_loss: 0.0168\n",
      "Epoch 22/50\n",
      "945/945 [==============================] - 65s 68ms/step - loss: 0.0069 - val_loss: 0.0167\n",
      "Epoch 23/50\n",
      "945/945 [==============================] - 65s 69ms/step - loss: 0.0069 - val_loss: 0.0164\n",
      "Epoch 24/50\n",
      "945/945 [==============================] - 66s 70ms/step - loss: 0.0069 - val_loss: 0.0162\n",
      "Epoch 25/50\n",
      "945/945 [==============================] - 66s 70ms/step - loss: 0.0068 - val_loss: 0.0160\n",
      "Epoch 26/50\n",
      "945/945 [==============================] - 71s 76ms/step - loss: 0.0068 - val_loss: 0.0157\n",
      "Epoch 27/50\n",
      "945/945 [==============================] - 79s 83ms/step - loss: 0.0068 - val_loss: 0.0155\n",
      "Epoch 28/50\n",
      "945/945 [==============================] - 72s 76ms/step - loss: 0.0068 - val_loss: 0.0154\n",
      "Epoch 29/50\n",
      "945/945 [==============================] - 78s 82ms/step - loss: 0.0068 - val_loss: 0.0153\n",
      "Epoch 30/50\n",
      "945/945 [==============================] - 73s 77ms/step - loss: 0.0068 - val_loss: 0.0153\n",
      "Epoch 31/50\n",
      "945/945 [==============================] - 68s 72ms/step - loss: 0.0068 - val_loss: 0.0151\n",
      "Epoch 32/50\n",
      "945/945 [==============================] - 53s 56ms/step - loss: 0.0068 - val_loss: 0.0148\n",
      "Epoch 33/50\n",
      "945/945 [==============================] - 53s 56ms/step - loss: 0.0067 - val_loss: 0.0150\n",
      "Epoch 34/50\n",
      "945/945 [==============================] - 61s 65ms/step - loss: 0.0067 - val_loss: 0.0148\n",
      "Epoch 35/50\n",
      "945/945 [==============================] - 63s 66ms/step - loss: 0.0067 - val_loss: 0.0146\n",
      "Epoch 36/50\n",
      "945/945 [==============================] - 57s 60ms/step - loss: 0.0067 - val_loss: 0.0146\n",
      "Epoch 37/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0067 - val_loss: 0.0144\n",
      "Epoch 38/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0067 - val_loss: 0.0149\n",
      "Epoch 39/50\n",
      "945/945 [==============================] - 61s 65ms/step - loss: 0.0067 - val_loss: 0.0148\n",
      "Epoch 40/50\n",
      "945/945 [==============================] - 61s 64ms/step - loss: 0.0067 - val_loss: 0.0149\n",
      "Epoch 41/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0066 - val_loss: 0.0148\n",
      "Epoch 42/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0066 - val_loss: 0.0148\n",
      "Epoch 43/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0066 - val_loss: 0.0147\n",
      "Epoch 44/50\n",
      "945/945 [==============================] - 51s 54ms/step - loss: 0.0066 - val_loss: 0.0143\n",
      "Epoch 45/50\n",
      "945/945 [==============================] - 52s 55ms/step - loss: 0.0066 - val_loss: 0.0146\n",
      "Epoch 46/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0066 - val_loss: 0.0146\n",
      "Epoch 47/50\n",
      "945/945 [==============================] - 63s 66ms/step - loss: 0.0066 - val_loss: 0.0145\n",
      "Epoch 48/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0066 - val_loss: 0.0145\n",
      "Epoch 49/50\n",
      "945/945 [==============================] - 64s 68ms/step - loss: 0.0066 - val_loss: 0.0144\n",
      "Epoch 50/50\n",
      "945/945 [==============================] - 64s 68ms/step - loss: 0.0066 - val_loss: 0.0146\n",
      "277/277 [==============================] - 9s 32ms/step\n",
      "Epoch 1/50\n",
      "945/945 [==============================] - 68s 70ms/step - loss: 0.0106 - val_loss: 0.0329\n",
      "Epoch 2/50\n",
      "945/945 [==============================] - 65s 69ms/step - loss: 0.0080 - val_loss: 0.0354\n",
      "Epoch 3/50\n",
      "945/945 [==============================] - 75s 80ms/step - loss: 0.0078 - val_loss: 0.0419\n",
      "Epoch 4/50\n",
      "945/945 [==============================] - 89s 94ms/step - loss: 0.0078 - val_loss: 0.0441\n",
      "Epoch 5/50\n",
      "945/945 [==============================] - 67s 71ms/step - loss: 0.0077 - val_loss: 0.0302\n",
      "Epoch 6/50\n",
      "945/945 [==============================] - 75s 79ms/step - loss: 0.0076 - val_loss: 0.0293\n",
      "Epoch 7/50\n",
      "945/945 [==============================] - 83s 88ms/step - loss: 0.0075 - val_loss: 0.0281\n",
      "Epoch 8/50\n",
      "945/945 [==============================] - 77s 81ms/step - loss: 0.0075 - val_loss: 0.0260\n",
      "Epoch 9/50\n",
      "945/945 [==============================] - 71s 76ms/step - loss: 0.0074 - val_loss: 0.0253\n",
      "Epoch 10/50\n",
      "945/945 [==============================] - 75s 80ms/step - loss: 0.0074 - val_loss: 0.0260\n",
      "Epoch 11/50\n",
      "945/945 [==============================] - 77s 82ms/step - loss: 0.0073 - val_loss: 0.0272\n",
      "Epoch 12/50\n",
      "945/945 [==============================] - 81s 86ms/step - loss: 0.0073 - val_loss: 0.0269\n",
      "Epoch 13/50\n",
      "945/945 [==============================] - 76s 81ms/step - loss: 0.0073 - val_loss: 0.0266\n",
      "Epoch 14/50\n",
      "945/945 [==============================] - 84s 88ms/step - loss: 0.0072 - val_loss: 0.0259\n",
      "Epoch 15/50\n",
      "945/945 [==============================] - 78s 83ms/step - loss: 0.0072 - val_loss: 0.0245\n",
      "Epoch 16/50\n",
      "945/945 [==============================] - 76s 80ms/step - loss: 0.0072 - val_loss: 0.0227\n",
      "Epoch 17/50\n",
      "945/945 [==============================] - 58s 62ms/step - loss: 0.0072 - val_loss: 0.0218\n",
      "Epoch 18/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0071 - val_loss: 0.0201\n",
      "Epoch 19/50\n",
      "945/945 [==============================] - 60s 64ms/step - loss: 0.0071 - val_loss: 0.0190\n",
      "Epoch 20/50\n",
      "945/945 [==============================] - 71s 75ms/step - loss: 0.0071 - val_loss: 0.0182\n",
      "Epoch 21/50\n",
      "945/945 [==============================] - 78s 82ms/step - loss: 0.0070 - val_loss: 0.0178\n",
      "Epoch 22/50\n",
      "945/945 [==============================] - 73s 77ms/step - loss: 0.0070 - val_loss: 0.0173\n",
      "Epoch 23/50\n",
      "945/945 [==============================] - 72s 76ms/step - loss: 0.0070 - val_loss: 0.0167\n",
      "Epoch 24/50\n",
      "945/945 [==============================] - 89s 94ms/step - loss: 0.0070 - val_loss: 0.0166\n",
      "Epoch 25/50\n",
      "945/945 [==============================] - 98s 104ms/step - loss: 0.0070 - val_loss: 0.0164\n",
      "Epoch 26/50\n",
      "945/945 [==============================] - 94s 99ms/step - loss: 0.0070 - val_loss: 0.0160\n",
      "Epoch 27/50\n",
      "945/945 [==============================] - 66s 70ms/step - loss: 0.0069 - val_loss: 0.0159\n",
      "Epoch 28/50\n",
      "945/945 [==============================] - 64s 68ms/step - loss: 0.0069 - val_loss: 0.0157\n",
      "Epoch 29/50\n",
      "945/945 [==============================] - 79s 84ms/step - loss: 0.0069 - val_loss: 0.0156\n",
      "Epoch 30/50\n",
      "945/945 [==============================] - 68s 72ms/step - loss: 0.0069 - val_loss: 0.0155\n",
      "Epoch 31/50\n",
      "945/945 [==============================] - 56s 59ms/step - loss: 0.0069 - val_loss: 0.0153\n",
      "Epoch 32/50\n",
      "945/945 [==============================] - 57s 60ms/step - loss: 0.0069 - val_loss: 0.0153\n",
      "Epoch 33/50\n",
      "945/945 [==============================] - 60s 63ms/step - loss: 0.0069 - val_loss: 0.0153\n",
      "Epoch 34/50\n",
      "945/945 [==============================] - 59s 63ms/step - loss: 0.0069 - val_loss: 0.0150\n",
      "Epoch 35/50\n",
      "945/945 [==============================] - 59s 63ms/step - loss: 0.0069 - val_loss: 0.0148\n",
      "Epoch 36/50\n",
      "945/945 [==============================] - 61s 64ms/step - loss: 0.0069 - val_loss: 0.0149\n",
      "Epoch 37/50\n",
      "945/945 [==============================] - 65s 69ms/step - loss: 0.0069 - val_loss: 0.0149\n",
      "Epoch 38/50\n",
      "945/945 [==============================] - 60s 63ms/step - loss: 0.0069 - val_loss: 0.0149\n",
      "Epoch 39/50\n",
      "945/945 [==============================] - 60s 63ms/step - loss: 0.0068 - val_loss: 0.0150\n",
      "Epoch 40/50\n",
      "945/945 [==============================] - 59s 63ms/step - loss: 0.0068 - val_loss: 0.0148\n",
      "Epoch 41/50\n",
      "945/945 [==============================] - 61s 64ms/step - loss: 0.0068 - val_loss: 0.0147\n",
      "Epoch 42/50\n",
      "945/945 [==============================] - 67s 71ms/step - loss: 0.0068 - val_loss: 0.0146\n",
      "Epoch 43/50\n",
      "945/945 [==============================] - 75s 79ms/step - loss: 0.0068 - val_loss: 0.0145\n",
      "Epoch 44/50\n",
      "945/945 [==============================] - 83s 88ms/step - loss: 0.0068 - val_loss: 0.0146\n",
      "Epoch 45/50\n",
      "945/945 [==============================] - 66s 70ms/step - loss: 0.0068 - val_loss: 0.0143\n",
      "Epoch 46/50\n",
      "945/945 [==============================] - 62s 65ms/step - loss: 0.0068 - val_loss: 0.0145\n",
      "Epoch 47/50\n",
      "945/945 [==============================] - 61s 64ms/step - loss: 0.0067 - val_loss: 0.0144\n",
      "Epoch 48/50\n",
      "945/945 [==============================] - 60s 64ms/step - loss: 0.0067 - val_loss: 0.0143\n",
      "Epoch 49/50\n",
      "945/945 [==============================] - 61s 64ms/step - loss: 0.0067 - val_loss: 0.0143\n",
      "Epoch 50/50\n",
      "945/945 [==============================] - 61s 65ms/step - loss: 0.0067 - val_loss: 0.0142\n",
      "277/277 [==============================] - 8s 26ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Metricas_Jd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_hat_TF_P,y_test_TF_P,history_TF_P, y_hatLSTM,y_testLSTM, historyLSTM, y_hat_STL_DP,y_test_STL_DP, history_STL_DP, Metricas \u001b[38;5;241m=\u001b[39m \u001b[43mTesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mventana\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprediccion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfechas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnodos1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnodos2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpaciencia\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepocas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivacion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivacion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactivacion3\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml2_norm_clip\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_microbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43msize_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43msize_test\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrueba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36mTesis\u001b[1;34m(datos, deltaf_p, b_p, ventana, prediccion, fechas, nodos1, nodos2, paciencia, epocas, batch, activacion1, activacion2, activacion3, l2_norm_clip, noise_multiplier, num_microbatches, learning_rate, size_test, size_val, nombre)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Realizar LSTM con datos STL_DP\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  \u001b[38;5;241m=\u001b[39m LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,size_test,size_val,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTL _DP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m MAES_LSTM,RMSE_LSTM,ER_Medianos_LSTM,ER_Medios_LSTM \u001b[38;5;241m=\u001b[39m \u001b[43mMetricas_Jd\u001b[49m(y_hatLSTM,y_testLSTM,prediccion)\n\u001b[0;32m     23\u001b[0m MAES_STL_DP,RMSE_STL_DP,ER_Medianos_STL_DP,ER_Medios_STL_DP \u001b[38;5;241m=\u001b[39m Metricas_Jd(y_hat_STL_DP,y_test_STL_DP,prediccion)\n\u001b[0;32m     24\u001b[0m MAES_TF_P,RMSE_TF_P,ER_Medianos_TF_P,ER_Medios_TF_P \u001b[38;5;241m=\u001b[39m Metricas_Jd(y_hat_TF_P,y_test_TF_P,prediccion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Metricas_Jd' is not defined"
     ]
    }
   ],
   "source": [
    "y_hat_TF_P,y_test_TF_P,history_TF_P, y_hatLSTM,y_testLSTM, historyLSTM, y_hat_STL_DP,y_test_STL_DP, history_STL_DP, Metricas = Tesis(datos,1,100,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate,size_test,size_test,\"Prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir modelos y hacer predicciones y métricas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Contugas-m7l_E1hd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
