{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Contugas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrir datos contugas\n",
    "\n",
    "new1 = pd.read_excel('EICH106.xlsx')\n",
    "new1.columns = ['VOLUMEN CORREGIDO', 'STD_VOLUME', 'ORIG_TEMPERATURE', 'TEMPERATURA','PRESION', 'ORIG_PRESSURE', 'VOLUMENSINCORREGIR', 'RAW_VOLUME', 'FECHAINICIAL']\n",
    "\n",
    "#función que pone las fechas en el mismo formato\n",
    "def cambiofecha(row):\n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        if isinstance(row.at[i, 'FECHAINICIAL'], str):\n",
    "            row.at[i, 'FECHAINICIAL'] = pd.to_datetime(row.at[i, 'FECHAINICIAL']).strftime('%Y-%m-%d %H:%M:%S')  \n",
    "        elif isinstance(row.at[i, 'FECHAINICIAL'], datetime):\n",
    "            row.at[i, 'FECHAINICIAL'] = datetime.strptime(str(row.at[i, 'FECHAINICIAL']),'%Y-%d-%m %H:%M:%S')\n",
    "            row.at[i, 'FECHAINICIAL'] = row.at[i, 'FECHAINICIAL'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return row\n",
    "\n",
    "new1=cambiofecha(new1)\n",
    "\n",
    "new1 = new1.set_index('FECHAINICIAL')\n",
    "new1.index = pd.to_datetime(new1.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# Función que añade al dataframe la hora, dia de la semana, mes y dia del año.\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features based on time series index.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    #df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month\n",
    "    #df['year'] = df.index.year\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    #df['dayofmonth'] = df.index.day\n",
    "    #df['weekofyear'] = df.index.isocalendar().week\n",
    "    return df\n",
    "\n",
    "\n",
    "new1 = create_features(new1)\n",
    "\n",
    "datos1=new1[[\"PRESION\", \"TEMPERATURA\", \"VOLUMENSINCORREGIR\", \"hour\", \"dayofweek\", \"month\", \"dayofyear\"]]\n",
    "\n",
    "\n",
    "#función que elimina las anomalias \n",
    "\n",
    "def eliminar_anomalias(df1,Vol,VolMin,VolMax,Temp,TempMin,TempMax,Presion,PresMin,PresMax):\n",
    "\n",
    "    df=df1.copy()\n",
    "\n",
    "    if Vol == True:\n",
    "        df[\"VOLUMENSINCORREGIR\"]= np.where((df[\"VOLUMENSINCORREGIR\"]<VolMin)|(df[\"VOLUMENSINCORREGIR\"]>VolMax),df['VOLUMENSINCORREGIR'].shift(168),df['VOLUMENSINCORREGIR'])\n",
    "\n",
    "    if Presion == True:\n",
    "        df[\"PRESION\"]= np.where((df[\"PRESION\"]<PresMin)|(df[\"PRESION\"]>PresMax),df['PRESION'].shift(168),df['PRESION'])\n",
    "\n",
    "    if Temp == True:\n",
    "        df[\"TEMPERATURA\"]= np.where((df[\"TEMPERATURA\"]<TempMin)|(df[\"TEMPERATURA\"]>TempMax),df['TEMPERATURA'].shift(168),df['TEMPERATURA'])\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "datos1=eliminar_anomalias(datos1,   True,0,250,True,17,35,True,14,19) ## Falta Presion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL - DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear función \n",
    "\n",
    "def STL_DP_S(datos,deltaf_p,b_p):\n",
    "\n",
    "    mstl = MSTL(datos, periods=[24, 24 * 7], iterate=5, stl_kwargs={\"seasonal_deg\": 0,\n",
    "                                                                            \"inner_iter\": 2,\n",
    "                                                                            \"outer_iter\": 0})\n",
    "    res = mstl.fit() # Use .fit() to perform and return the decomposition\n",
    "    #ax = res.plot()\n",
    "    #plt.tight_layout()\n",
    "\n",
    "\n",
    "    res.trend\n",
    "\n",
    "    tendencia = res.trend\n",
    "    seasonal = res.seasonal\n",
    "    residual = res.resid\n",
    "\n",
    "\n",
    "    tendenciaFourier = np.fft.fft(tendencia)\n",
    "\n",
    "    # Generar el ruido Laplaciano y aplicarlo a los coeficientes de Fourier\n",
    "    b = b_p\n",
    "    deltaf = deltaf_p\n",
    "    epsilon = deltaf/ b\n",
    "\n",
    "    # loc = media, scale = b\n",
    "    laplace = np.random.laplace(loc=0, scale=1/epsilon,size = tendenciaFourier.shape )\n",
    "\n",
    "    #laplace_noise = np.random.laplace(loc=0, scale=b, size=tendenciaFourier.shape)\n",
    "    perturbed_trend_dft = tendenciaFourier + laplace\n",
    "\n",
    "\n",
    "    # \n",
    "    perturbed_trend = np.fft.ifft(perturbed_trend_dft).real\n",
    "\n",
    "\n",
    "    # sacar datos de ruido\n",
    "    DatosRuido = perturbed_trend + seasonal['seasonal_168'] + seasonal['seasonal_24'] + residual\n",
    "\n",
    "\n",
    "\n",
    "    return DatosRuido,epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3):\n",
    "\n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=0.1, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = 0.2, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas sin PD.keras'\n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,norm_clip,ruido,microBatches,lr):\n",
    "    \n",
    "\n",
    "    datos = datos.values.reshape(-1, 1)\n",
    "\n",
    "    estandarizacion = MinMaxScaler().fit(datos)\n",
    "    scaled_data = estandarizacion.transform(datos)\n",
    "\n",
    "\n",
    "    # dividir en train, test\n",
    "    X, y = [], []\n",
    "    Xf,yf = [],[]\n",
    "\n",
    "    for i in range(len(scaled_data) - ventana - prediccion):\n",
    "        X.append(scaled_data[i:i+ventana])\n",
    "        y.append(scaled_data[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "        Xf.append(fechas[i:i+ventana])\n",
    "        yf.append(fechas[i+ventana:i+ventana+prediccion])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    Xf,yf = np.array(Xf),np.array(yf)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    fecha_X_train, fecha_X_test, fecha_y_train, fecha_y_test = train_test_split(Xf, yf, test_size=0.1, shuffle=False)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(nodos1,activation= activacion1, input_shape=(ventana,1)))\n",
    "    model.add(Dense(nodos2, activation=activacion2))\n",
    "    model.add(Dense(prediccion , activation=activacion3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if batch % microBatches != 0:\n",
    "        raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "\n",
    "\n",
    "    # agregar la privacidad diferencial en el optimizador \n",
    "    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "        l2_norm_clip=norm_clip,\n",
    "        noise_multiplier=ruido,\n",
    "        num_microbatches=microBatches,\n",
    "        learning_rate=lr)\n",
    "\n",
    "    # Función de pérdida para regresión\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=paciencia, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epocas,validation_split = 0.2, verbose=1, batch_size=batch,shuffle = False, callbacks=[early_stopping])\n",
    "\n",
    "    # guardar los archivo a usar en la carpeta \n",
    "    rutaAGuardar = f'Modelo {nodos1} - nodos1 - {nodos2} - nodos2 - {epocas} Epocas sin PD.keras'\n",
    "    model.save(rutaAGuardar)\n",
    "\n",
    "\n",
    "        \n",
    "    y_hat = model.predict(X_test, verbose=1)\n",
    "    y_hat = estandarizacion.inverse_transform(y_hat)\n",
    "\n",
    "    y_test1 = y_test.reshape(-1, 1)\n",
    "\n",
    "    y_test1 = estandarizacion.inverse_transform(y_test1)\n",
    "\n",
    "    y_test1 = y_test1.reshape(-1,24,1)\n",
    "\n",
    "\n",
    "    return y_hat,y_test1,fecha_y_test, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar STL-DP a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos, DeltaF, B\n",
    "DatosRuido,epsilon = STL_DP_S(datos1['TEMPERATURA'],1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(datos1['TEMPERATURA'][:168].values)\n",
    "plt.plot(DatosRuido.values[:168])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correr Red Neuronal LSTM Con Datos Perturbados y normales y correr TF-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "\n",
    "#datos = datos1['TEMPERATURA'].values.reshape(-1, 1)\n",
    "ventana = 168\n",
    "prediccion = 24\n",
    "fechas = datos1.index\n",
    "nodos1 = 100\n",
    "nodos2 = 100\n",
    "paciencia = 10\n",
    "epocas = 20\n",
    "batch = 32\n",
    "activacion1 = \"tanh\"\n",
    "activacion2 = \"tanh\"\n",
    "activacion3 = \"linear\"\n",
    "\n",
    "l2_norm_clip = 0\n",
    "noise_multiplier = 10\n",
    "num_microbatches = 4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos1['TEMPERATURA'],ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P = LSTM_TFP_JD(datos1['TEMPERATURA'],ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacidad TF_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función que haga todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tesis(datos,deltaf_p,b_p,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate):\n",
    "\n",
    "    # Realizar STL-DP\n",
    "    DatosRuido,epsilon_STL_DP = STL_DP_S(datos,deltaf_p,b_p)\n",
    "\n",
    "    # Realizar LSTM con datos sin ruido\n",
    "    y_hatLSTM,y_testLSTM,fecha_y_testLSTM, historyLSTM = LSTM_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)\n",
    "    \n",
    "    # Realizar LSTM con datos STL_DP\n",
    "    y_hat_STL_DP,y_test_STL_DP,fecha_y_test_STL_DP, history_STL_DP  = LSTM_JD(DatosRuido,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3)\n",
    "\n",
    "    # Realizar LSTM con TF_P\n",
    "    y_hat_TF_P,y_test_TF_P,fecha_y_test_TF_P = LSTM_TFP_JD(datos,ventana,prediccion,fechas,nodos1,nodos2,paciencia,epocas,batch,activacion1,activacion2,activacion3,l2_norm_clip,noise_multiplier,num_microbatches,learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Contugas-m7l_E1hd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
